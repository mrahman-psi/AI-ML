{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11568812,"sourceType":"datasetVersion","datasetId":7253205},{"sourceId":11569667,"sourceType":"datasetVersion","datasetId":7253605},{"sourceId":11910577,"sourceType":"datasetVersion","datasetId":7487776},{"sourceId":12038896,"sourceType":"datasetVersion","datasetId":7377931}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install monai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T02:08:28.795945Z","iopub.execute_input":"2025-07-02T02:08:28.796288Z","iopub.status.idle":"2025-07-02T02:09:59.956605Z","shell.execute_reply.started":"2025-07-02T02:08:28.796264Z","shell.execute_reply":"2025-07-02T02:09:59.955031Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.5.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.24->monai) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed monai-1.5.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport monai\nRUN_TRAIN = False # bfloat16 or float32 recommended\nRUN_VALID = True\nRUN_TEST  = True\n\nif not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n    raise RuntimeError(\"Requires >= 2 GPUs with CUDA enabled.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T02:10:33.784101Z","iopub.status.idle":"2025-07-02T02:10:33.784521Z","shell.execute_reply.started":"2025-07-02T02:10:33.784326Z","shell.execute_reply":"2025-07-02T02:10:33.784345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Data Analysis\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\nimport os\nimport torch\nimport uuid\n# ---------------------\n# Device Setup\n# ---------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\noutput_dir = \"/kaggle/working/\"\n\n\n# Plot the velocity map\ndef plot_velocity(velocity, sample):\n    fig, ax = plt.subplots(1, 1, figsize=(11, 5))\n    img = ax.imshow(velocity[sample, 0, :, :], cmap='jet')\n    ax.set_xticks(range(0, 70, 10))\n    ax.set_xticklabels(range(0, 700, 100))\n    ax.set_yticks(range(0, 70, 10))\n    ax.set_yticklabels(range(0, 700, 100))\n    ax.set_ylabel('Depth (m)', fontsize=12)\n    ax.set_xlabel('Offset (m)', fontsize=12)\n    clb = plt.colorbar(img, ax=ax)\n    clb.ax.set_title('km/s', fontsize=8)\n    plt.savefig(os.path.join(output_dir, f\"velocity_map_sample_{sample}.png\"))\n    plt.close()\n    plt.figure(figsize=(8, 2.5))\n    plt.plot(np.arange(5, 700, 10), np.mean(velocity[sample, 0, :, :], axis=1))\n    plt.xlabel(\"Depth (m)\")\n    plt.ylabel(\"Ave Velocity (m/s)\")\n    plt.savefig(os.path.join(output_dir, f\"velocity_profile_sample_{sample}.png\"))\n    plt.close()\n\n# Get information from the velocity map\ndef info_velocity(velocity, sample, for_show=True):\n    ave_vel = np.mean(velocity[sample, 0, :, :])\n    min_vel = np.min(velocity[sample, 0, :, :])\n    max_vel = np.max(velocity[sample, 0, :, :])\n    medi_vel = np.median(velocity[sample, 0, :, :])\n    num_vels = len(np.unique(velocity[sample, 0, :, :]))\n    y0_velL = np.mean(velocity[sample, 0, 0, 0:35])\n    y0_velR = np.mean(velocity[sample, 0, 0, 35:])\n    y09L_medi = np.median(velocity[sample, 0, 0:10, 0:35])\n    y09R_medi = np.median(velocity[sample, 0, 0:10, 35:])\n    y1029_medi = np.median(velocity[sample, 0, 10:30, :])\n    y3049_medi = np.median(velocity[sample, 0, 30:50, :])\n    y5069_medi = np.median(velocity[sample, 0, 50:, :])\n    y09L_mean = np.mean(velocity[sample, 0, 0:10, 0:35])\n    y09R_mean = np.mean(velocity[sample, 0, 0:10, 35:])\n    y1029_mean = np.mean(velocity[sample, 0, 10:30, :])\n    y3049_mean = np.mean(velocity[sample, 0, 30:50, :])\n    y5069_mean = np.mean(velocity[sample, 0, 50:, :])\n    if for_show:\n        print(\"Number of distinct velocities: {}\".format(num_vels))\n        print(\"Average velocity: {:.2f} m/s\".format(ave_vel))\n        print(\" Median velocity: {:.2f} m/s\".format(medi_vel),\n              \"Min, Max: {:.2f}, {:.2f}\".format(min_vel, max_vel))\n        print(\"Ave y=0 velocities L,R: {:.2f}, {:.2f}\".format(y0_velL, y0_velR))\n        print(\"Median velocities in rows:  {:.2f}(0-9:L), {:.2f}(0-9:R),\".format(\n            y09L_medi, y09R_medi),\n            \"{:.2f}(10-29), {:.2f}(30-49), {:.2f}(50-69)\".format(\n            y1029_medi, y3049_medi, y5069_medi))\n        print(\"  Mean velocities in rows:  {:.2f}(0-9:L), {:.2f}(0-9:R),\".format(\n            y09L_mean, y09R_mean),\n            \"{:.2f}(10-29), {:.2f}(30-49), {:.2f}(50-69)\".format(\n            y1029_mean, y3049_mean, y5069_mean))\n    else:\n        return (num_vels, y0_velL, y0_velR, y09L_medi, y09R_medi,\n                y1029_medi, y3049_medi, y5069_medi)\n\n# Make a gray-scale image of the seismic data\ndef plot_data(data, sample=-1):\n    fig, ax = plt.subplots(1, 5, figsize=(20, 7))\n    if len(data.shape) == 3:\n        thisdata = data[:, :, :]\n    else:\n        thisdata = data[sample, :, :, :]\n    maxabs = []\n    for srclocid, xloc in enumerate([0, 17, 34, 52, 69]):\n        maxabs.append(np.max(np.abs(thisdata[srclocid, 180:, xloc])))\n    vrange = np.max(maxabs) * 0.5\n    for iax in range(5):\n        ax[iax].imshow(thisdata[iax, :, :], extent=[0, 70, 1000, 0],\n                       aspect='auto', cmap='gray', vmin=-vrange, vmax=vrange)\n    for axis in ax:\n        axis.set_xticks(range(0, 70, 10))\n        axis.set_xticklabels(range(0, 700, 100))\n        axis.set_yticks(range(0, 2000, 1000))\n        axis.set_yticklabels(range(0, 2, 1))\n        axis.set_ylabel('Time (s)', fontsize=12)\n        axis.set_xlabel('Offset (m)', fontsize=12)\n    plt.savefig(os.path.join(output_dir, f\"seismic_data_sample_{sample}.png\"))\n    plt.close()\n\ndef time_max_peak(isrc, xloc, thisdata):\n    ipeak = np.argmax(thisdata[isrc, :, xloc])\n    peakvals = thisdata[isrc, ipeak-3:ipeak+4, xloc]\n    timevals = np.linspace(ipeak-3, ipeak+3, num=7, endpoint=True)\n    if len(peakvals) == len(timevals):\n        fitcoefs = np.poly1d(np.polyfit(timevals, peakvals, 2)).coef\n        return -0.5 * fitcoefs[1] / fitcoefs[0]\n    else:\n        print(\"mis-matched lengths:\\n\", timevals, \"\\n\", peakvals)\n        return ipeak\n\ndef info_data(data, sample=-1, for_show=True):\n    if len(data.shape) == 3:\n        thisdata = data[:, :, :]\n    else:\n        thisdata = data[sample, :, :, :]\n    partdata = thisdata[:, 0:225, :]\n    vsurfaceL = 4*5*10*1000 / (\n        time_max_peak(0, 6, partdata) - time_max_peak(0, 1, partdata) +\n        time_max_peak(1, 11, partdata) - time_max_peak(1, 16, partdata) +\n        time_max_peak(1, 23, partdata) - time_max_peak(1, 18, partdata) +\n        time_max_peak(2, 28, partdata) - time_max_peak(2, 33, partdata)\n    )\n    vsurfaceR = 4*5*10*1000 / (\n        time_max_peak(2, 40, partdata) - time_max_peak(2, 35, partdata) +\n        time_max_peak(3, 46, partdata) - time_max_peak(3, 51, partdata) +\n        time_max_peak(3, 58, partdata) - time_max_peak(3, 53, partdata) +\n        time_max_peak(4, 63, partdata) - time_max_peak(4, 68, partdata)\n    )\n    vsurface = (vsurfaceL + vsurfaceR) / 2\n    if for_show:\n        idists = np.arange(0, 70)\n        dists = []\n        times = []\n        timeref = np.argmax(thisdata[2, :, 34])\n        for idist in idists:\n            dists.append(10 * idist)\n            times.append(np.sign(idist-34) * (np.argmax(thisdata[2, :, idist]) - timeref) / 1000.0)\n        times = -1.0 * (np.array(times) - times[0])\n        plt.figure(figsize=(6, 3))\n        plt.plot(dists, times, '.b', alpha=0.7)\n        plt.plot([dists[0], dists[-1]], [times[0], times[-1]], c='orange', alpha=0.6)\n        plt.ylabel(\"$-$ Time (s)\")\n        plt.xlabel(\"Surface Distance (m)\")\n        plt.title(\"Time vs Distance from Source 2\")\n        plt.savefig(os.path.join(output_dir, f\"time_vs_distance_sample_{np.random.randint(100000)}.png\"))\n        plt.close()\n        print(\"Surface velocities : {:.2f}-Left, {:.2f}-Average, {:.2f}-Right\".format(\n            vsurfaceL, vsurface, vsurfaceR))\n    else:\n        return vsurfaceL, vsurface, vsurfaceR\n\ndef sources_data(data, sample=-1, for_show=True):\n    if len(data.shape) == 3:\n        thisdata = data[:, :, :]\n    else:\n        thisdata = data[sample, :, :, :]\n    maxamps = []\n    minamps = []\n    for srclocid, xloc in enumerate([0, 17, 34, 52, 69]):\n        maxamps.append(np.max(thisdata[srclocid, 180:, xloc]))\n        minamps.append(np.min(thisdata[srclocid, 180:, xloc]))\n    max_amp = np.max(maxamps)\n    min_amp = np.min(minamps)\n    delta_amp = 0.05 * (max_amp - min_amp)\n    plt.figure(figsize=(8, 5))\n    for srclocid, xloc in enumerate([0, 17, 34, 52, 69]):\n        timeseries = thisdata[srclocid, :, xloc]\n        offset = delta_amp * (xloc - 34) / 35.0\n        plt.plot(np.array(range(1000)), timeseries + offset, alpha=0.7)\n    plt.plot([0, 1000], [0.0, 0.0], c='gray', alpha=0.5)\n    plt.ylim(1.10 * min_amp - delta_amp, 1.10 * max_amp + delta_amp)\n    plt.xlabel('Time (ms)')\n    plt.ylabel(\"Amplitude     Traces are offset.\")\n    plt.title(\"Waveforms at the 5 source locations\")\n    # Save the plot to output_dir with a unique name\n    unique_name = f\"waveforms_{uuid.uuid4().hex[:8]}.png\"\n    plt.savefig(os.path.join(output_dir, unique_name))\n    plt.close()\n\n# Look into Train_sample folder and visualize the data\n\nlast_data_file = \"None\"\n\ndef get_train_sample(dfind, ftscale=True):\n    global velocity, data, last_data_file\n    #D:\\Personal\\Mamun\\Training\\AI ML Bootcamp\\Python\\Workspace\\Yale Project\\train_samples\n    train_dir = \"/kaggle/input/waveform-inversion/train_samples/\"\n    output_dir = \"/kaggle/working/images/\"\n    veltype, ifile, isample = traindf.loc[dfind, [\"veltype\", \"ifile\", \"isample\"]]\n    if (\"Vel\" in veltype) or (\"Style\" in veltype):\n        data_file = train_dir + veltype + \"/data/data\" + str(ifile) + \".npy\"\n        model_file = train_dir + veltype + \"/model/model\" + str(ifile) + \".npy\"\n    else:\n        fault_num = 2 * ifile + 4 * (\"_B\" in veltype)\n        data_file = train_dir + veltype + \"/seis\" + str(fault_num) + \"_1_0.npy\"\n        model_file = train_dir + veltype + \"/vel\" + str(fault_num) + \"_1_0.npy\"\n    if data_file != last_data_file:\n        data = np.load(data_file)\n        if ftscale:\n            for itime in range(1000):\n                data[:, :, itime, :] = (1.0 + (itime / 200) ** 1.5) * data[:, :, itime, :]\n        velocity = np.load(model_file)\n        last_data_file = data_file\n    return velocity, data, isample\n\n# Make a dataframe with 10,000 rows labeled by:\n#   type - 5 x 2 string values\n#   ifile - two numeric values: 0,1 or 1,2 or 2,4 or 6,8 depending on type\n#   isample - 0 to 499\nveltypes = [\"FlatVel\",\"FlatFault\", \"CurveVel\", \"CurveFault\", \"Style\"]\nveltype = []; ifile = []; isample = []\nfor this_type in veltypes:\n    for this_AB in [\"_A\",\"_B\"]:\n        for this_ifile in [1,2]:\n            for this_isample in range(500):  # **************************************\n                veltype.append(this_type+this_AB); ifile.append(this_ifile); isample.append(this_isample)\n# Make a dataframe from these\ntraindf = pd.DataFrame({\"veltype\":veltype, \"ifile\":ifile, \"isample\":isample})\n# Select a dataframe index to look at\ndfind = int(0.87*len(traindf))\n\n\nprint(list(traindf.loc[dfind,[\"veltype\",\"ifile\",\"isample\"]]))\nvelocity, data, isample = get_train_sample(dfind)\n\nprint('Velocity map size:', velocity.shape)\nprint('Seismic data size:', data.shape)\n\n# Look at the velocity map for the training sample\n# isample defined above\nplot_velocity(velocity, isample)\ninfo_velocity(velocity, isample)\n\n# Look at the seismic data for the sample\n# isample = same as for the velocity map above\nplot_data(data, isample)\ninfo_data(data, isample)\nsources_data(data, isample)\n\n# Investigate Test Data\n# Look at one of them (they seem to be shuffled)\n##testdata = np.load('/kaggle/input/waveform-inversion/test/000039dca2.npy')  # messy\n##testdata = np.load('/kaggle/input/waveform-inversion/test/0001026c8a.npy')  # very simple\ntestdata = np.load('/kaggle/input/waveform-inversion/test/00015b24d5.npy')  # weird straight lines\n##testdata = np.load('/kaggle/input/waveform-inversion/test/800222ab0d.npy')  # messy\n##testdata = np.load('/kaggle/input/waveform-inversion/test/a00269f1eb.npy')  # messy\n##testdata = np.load('/kaggle/input/waveform-inversion/test/c0021521e5.npy')  # simple-ish\n\n# Scale the seismic data by ~ (1+(t/a)^b) to help equalize the amplitudes vs time.\n# (This is similar to applying AGC for visualization, but is included in the analysis too.)\nfor itime in range(1000):\n    testdata[ : , itime, : ] = (1.0+(itime/200)**1.5)*testdata[ : , itime, : ]\n\nprint('Test data size:', testdata.shape)\n\nplot_data(testdata)\ninfo_data(testdata)\nsources_data(testdata)\n\n# For each sample,\n# add y_ targets: y0_velL, y0_velR, y09L_medi, y09R_medi, y1039_medi, y4069_medi\nnunique = []; y0_aves = []; y0_diffs = []\ny09L_medis = []; y09R_medis = []; y1029_medis = []; y3049_medis = []; y5069_medis = []\n# add x_ features: surface velocity average and R-L difference\nsurf_aves = []; surf_diffs = []\nfor dfind in traindf.index:\n    velocity, data, isample = get_train_sample(dfind, ftscale=False)\n    # velocity, target, values\n    (num_vels, y0_velL, y0_velR, y09L_medi, y09R_medi,\n         y1029_medi, y3049_medi, y5069_medi) = info_velocity(\n                                            velocity, isample, for_show=False)\n    nunique.append(num_vels)\n    y0_aves.append((y0_velL + y0_velR)/2); y0_diffs.append(y0_velR - y0_velL)\n    y09L_medis.append(y09L_medi); y09R_medis.append(y09R_medi); y1029_medis.append(y1029_medi)\n    y3049_medis.append(y3049_medi); y5069_medis.append(y5069_medi)\n    # seismic, feature, values\n    velL, velave, velR = info_data(data, isample, for_show=False)\n    surf_aves.append(velave)\n    surf_diffs.append(velR-velL)\ntraindf[\"y_numVels\"] = nunique\ntraindf[\"y_y0Ave\"] = y0_aves\ntraindf[\"y_y0Diff\"] = y0_diffs\ntraindf[\"y_09LMedi\"] = y09L_medis\ntraindf[\"y_09RMedi\"] = y09R_medis\ntraindf[\"y_1029Medi\"] = y1029_medis\ntraindf[\"y_3049Medi\"] = y3049_medis\ntraindf[\"y_5069Medi\"] = y5069_medis\ntraindf[\"x_surfAve\"] = surf_aves\ntraindf[\"x_surfDiff\"] = surf_diffs\n\n# Add color-coding based on the surfDiff and surfAve values\n# Red = R-L not zero; Blue = R-L near zero\ntraindf[\"diff_clr\"] = 'red'\n# Use measured R-L difference to set color\nseldiff = traindf[\"x_surfAve\"] > (1300.0 + 1200.0*np.log10(1+np.abs(traindf[\"x_surfDiff\"])))\n# Use known R-L difference from the target (can't do this for test)\n##seldiff = (np.abs(traindf[\"y_y0Diff\"]) < 0.1*diff_color_change)\ntraindf.loc[seldiff, \"diff_clr\"] = 'blue'\n\n# Print the dataframe\nprint(traindf.head(10))\n\n# Summary values for the columns\ntraindf_means = traindf.describe().loc[\"mean\",]\ntraindf.describe()\n\n# Save the Training Dataframe\ntraindf.to_csv(\"traindf.csv\", header=True, index=False, float_format='%.2f')\n# For plots\nvelocity_range = (1400,4600)\nprint(\"\\nMedian Ave Surface Velocity: {:.2f}\".format(np.median(traindf[\"x_surfAve\"])))\nprint(\"Average Ave Surface Velocity: {:.2f}\\n\".format(np.mean(traindf[\"x_surfAve\"])))\n\ndiffs = traindf[\"x_surfDiff\"]\n\nplt.figure(figsize=(8,4))\nplt.hist(traindf[\"x_surfAve\"],bins=100)\nplt.title(\"Train: Histogram of the Average Surface Velocity\")\nplt.xlabel(\"Surface Velocity (m/s)\")\nplt.xlim(velocity_range)\nplt.savefig(os.path.join(output_dir, f\"train_hist_surface_velocity.png\"))\nplt.show()\n\nplt.figure(figsize=(8,4))\nplt.hist(np.sign(diffs)*np.log10(np.abs(diffs) + 1.0), log=True, bins=100)\nplt.title(\"Train: Histogram of the R-L Velocity Difference\")\nplt.xlabel(\"Signed Log10[1+ R-L Surface Velocity Difference (m/s) ]\")\nplt.savefig(os.path.join(output_dir, f\"train_hist_velocity_difference.png\"))\nplt.show()\n\n\n\nplt.figure(figsize=(8,5))\nplt.scatter( np.sign(diffs)*(np.log10(np.abs(diffs) + 1.0)), traindf[\"x_surfAve\"],\n                             color=traindf[\"diff_clr\"], s=2, alpha=0.25)\nlindiffs = np.linspace(-3.0,3.0,100)  # <-- This is log10(1+ abs(surfDiff) )\nplt.plot(lindiffs, 1300.0 + 1200.0*np.abs(lindiffs),c='gray',alpha=0.5)\nplt.ylabel(\"Average Surface Velocity (m/s)\")\nplt.xlabel(\"Signed Log10[1+ R-L Velocity Difference (m/s) ]\")\nplt.title(\"Train: Average Surface Velocity vs. R-L Velocity Difference\")\nplt.ylim(velocity_range)\nplt.savefig(os.path.join(output_dir, f\"train_scatter_velocity_vs_difference.png\"))  \nplt.show()\n\n# Look into the y=0 Average and Difference\n\n# Scatter plot of the y=0 row Average and y=0 R-L Difference velocities\nif True:\n    diffs = traindf[\"y_y0Diff\"]\n\n    plt.figure(figsize=(6,3))\n    plt.scatter( np.sign(diffs)*(np.log10(np.abs(diffs) + 1.0)), traindf[\"y_y0Ave\"]/1000,\n                             color=traindf[\"diff_clr\"], s=2, alpha=0.25)\n    plt.ylabel(\"Ave y=0 Velocity (km/s)\")\n    plt.xlabel(\"Signed Log10[1+ y=0 R-L Velocity Diff (m/s) ]\")\n    plt.title(\"Train: y=0 Average Velocity vs. y=0 R-L Velocity Difference\")\n    #plt.savefig(os.path.join(output_dir, f\"train_y0_scatter_velocity_vs_diff.png\"))\n    plt.ylim(1.4,4.6) # in km/s\n    plt.savefig(os.path.join(output_dir, f\"train_y0_scatter_velocity_vs_diff.png\"))\n    plt.show()\n\n    # Histogra of the y=0 R-L Diff\n    plt.figure(figsize=(6,3))\n    plt.hist(np.sign(diffs)*np.log10(np.abs(diffs) + 1.0), log=True, bins=100)\n    plt.title(\"Train: Histogram of the y=0 R-L Velocity Difference\")\n    plt.xlabel(\"Signed Log10[1+ R-L y=0 Velocity Difference (m/s) ]\")\n    plt.savefig(os.path.join(output_dir, f\"train_hist_y0_difference.png\"))  \n    plt.show()\n\n    # Scatter plot of the Seismic R-L Diff vs the y=0 R-L Diff\n    diffs = traindf[\"x_surfDiff\"]\n    diffy0 = traindf[\"y_y0Diff\"]\n\n    plt.figure(figsize=(6,3))\n    plt.scatter( np.sign(diffy0)*(np.log10(np.abs(diffy0) + 1.0)),\n                    np.sign(diffs)*(np.log10(np.abs(diffs) + 1.0)),\n                             color=traindf[\"diff_clr\"], s=2, alpha=0.25)\n    plt.xlabel(\"y=0  Log10[1+ R-L Velocity Diff (m/s) ]\")\n    plt.ylabel(\"Seismic  Log10[1+ R-L Velocity Diff (m/s) ]\")\n    plt.title(\"Train: Seismic R-L Difference vs the y=0 R-L Difference\")\n    plt.savefig(os.path.join(output_dir, f\"train_y0_scatter_velocity_vs_diff.png\"))\n    plt.show()\n# Find some with y=0 diff = 0 and yet seismic R-L is high\n##traindf[(traindf[\"y_y0Diff\"] == 0) & (traindf[\"x_surfDiff\"] > 200)]\n\n# Simple degree 1 polynomial fit \nmodel = np.poly1d(np.polyfit(np.array(traindf[\"y_y0Ave\"]), \n                             np.array(traindf[\"x_surfAve\"]), 1))\n# for polynomial line visualization \npolyline = np.linspace(1400, 4500, 100)  \n\nplt.figure(figsize=(4,4))\nplt.scatter( traindf[\"y_y0Ave\"], traindf[\"x_surfAve\"],\n                color=traindf[\"diff_clr\"], s=2, alpha=0.25)\nplt.plot(polyline, model(polyline), c='orange',alpha=0.6)\nplt.xlabel(\"y=0 Average Velocity\")\nplt.ylabel(\"Seismic Average Surface Velocity (m/s)\")\nplt.title(\"Train: Seismic Surface Velocity vs. y=0 Velocity\")\nplt.xlim(velocity_range)\nplt.ylim(velocity_range)\nplt.savefig(os.path.join(output_dir, f\"train_surf_vs_y0.png\"))\nplt.show()\n\nprint(\"   Fit coefs [slope, intercept]:\", model.coef,\"\\n\")\n\n# Compare the velocities in row ranges vs the surface velocity and velocity difference.\n# Create simple model fits for each region.\n\nsurfAves = traindf[\"x_surfAve\"]\nsurfDiffs = traindf[\"x_surfDiff\"]\nlog_surfDiffs = np.sign(surfDiffs)*(np.log10(np.abs(surfDiffs) + 1.0))\n\n# Fit red, blue separately, limit the surfAve range used\nselblue = (traindf[\"diff_clr\"] == 'blue') & (traindf[\"x_surfAve\"] < 4000)\nselred = (traindf[\"diff_clr\"] == 'red') & (traindf[\"x_surfAve\"] < 4000)\n\n# for polynomial line visualization \npolyline = np.linspace(1400, 4000, 100)\n\n# Save the fit models\nrows_models = []\nfor y_rows in [\"09L\", \"09R\", \"1029\", \"3049\", \"5069\"]:\n    \n    rows_values = traindf[\"y_\"+y_rows+\"Medi\"]\n    surf_values = surfAves.copy()\n    vel_axis_label = \"Ave Surface Velocity (m/s)\"\n    # Modify surf_values for the 09L,R data\n    if \"09L\" in y_rows:\n        surf_values = surf_values - 0.5*surfDiffs\n        vel_axis_label = \"L Surface Velocity (m/s)\"\n    if \"09R\" in y_rows:\n        surf_values = surf_values + 0.5*surfDiffs\n        vel_axis_label = \"R Surface Velocity (m/s)\"\n   \n    plt.figure(figsize=(7,4))\n    plt.scatter(surf_values, rows_values, color=traindf[\"diff_clr\"], s=2, alpha=0.25)\n\n    degree = 3\n    # Blue polynomial fit:\n    if \"09\" in y_rows:\n        # Use combined L and R data for the model, selblue:\n        surf_RLvalues = np.concatenate( ( (surfAves - 0.5*surfDiffs)[selblue], \n                                            (surfAves + 0.5*surfDiffs)[selblue] ) )\n        rows_RLvalues = np.concatenate( ( traindf.loc[selblue,\"y_09LMedi\"], \n                                            traindf.loc[selblue,\"y_09RMedi\"] ) )\n        model = np.poly1d(np.polyfit(surf_RLvalues, \n                                        rows_RLvalues, degree))\n    else:\n        model = np.poly1d(np.polyfit(np.array(surf_values[selblue]), \n                             np.array(rows_values[selblue]), degree))\n    \n    rows_models.append(model)\n    blue_resids = (-1.0*model(np.array(surf_values[selblue])) + \n                             np.array(rows_values[selblue]))\n    print(\"  Blue Fit coefs:\", model.coef)\n    plt.plot(polyline, model(polyline), c='blue',alpha=0.6)\n    #\n    # Red polynomial fit:\n    if \"09\" in y_rows:\n        # Use combined L and R data for the model, selred:\n        surf_RLvalues = np.concatenate( ( (surfAves - 0.5*surfDiffs)[selred], \n                                            (surfAves + 0.5*surfDiffs)[selred] ) )\n        rows_RLvalues = np.concatenate( ( traindf.loc[selred,\"y_09LMedi\"], \n                                            traindf.loc[selred,\"y_09RMedi\"] ) )\n        model = np.poly1d(np.polyfit(surf_RLvalues, \n                                        rows_RLvalues, degree))\n    else:\n        model = np.poly1d(np.polyfit(np.array(surf_values[selred]), \n                             np.array(rows_values[selred]), degree))\nrows_models.append(model)\nred_resids = (-1.0*model(np.array(surf_values[selred])) + \n                             np.array(rows_values[selred]))\nprint(\"  Red Fit coefs:\", model.coef)\nplt.plot(polyline, model(polyline), c='purple',alpha=0.6)\n\nplt.xlabel(vel_axis_label)\nplt.xlim(1400, 4100) # reduce because of fitting range\nplt.ylabel(\"y_\"+y_rows+\" Median\")\nplt.ylim(velocity_range)\nplt.title(\"Train: y_\"+y_rows+\" Median vs. Surface Velocity\")\nplt.savefig(os.path.join(output_dir, f\"train_rows\"+y_rows+\"_vs_average.png\"))\nplt.show()\n\n\n# Show the residuals vs surface difference for the 09L, 09R\nif \"09\" in y_rows:\n    plt.figure(figsize=(7,2))\n    plt.scatter( log_surfDiffs[selblue], blue_resids,\n                            color=traindf.loc[selblue,\"diff_clr\"], s=2, alpha=0.25)\n    plt.scatter( log_surfDiffs[selred], red_resids,\n                            color=traindf.loc[selred,\"diff_clr\"], s=2, alpha=0.25)\n    plt.ylim(-1000,1000)\n    plt.xlabel(\"Signed Log10[1+ R-L Velocity Diff (m/s) ]\")\n    plt.ylabel(\"y_\"+y_rows+\" Residuals\")\n    plt.title(\"Train: y_\"+y_rows+\" * Residuals * vs. Surface Difference\")\n    plt.savefig(os.path.join(output_dir, f\"train_residuals\"+y_rows+\"_vs_difference.png\"))\n    plt.show()\n    \n    \n# Show the median values vs surface difference\nplt.figure(figsize=(7,2))\nplt.scatter(log_surfDiffs, rows_values,\n                            color=traindf[\"diff_clr\"], s=2, alpha=0.25)\nplt.xlabel(\"Signed Log10[1+ R-L Velocity Diff (m/s) ]\")\nplt.ylabel(\"y_\"+y_rows+\" Median\")\nplt.ylim(velocity_range)\nplt.title(\"Train: y_\"+y_rows+\" Median vs. Surface Difference\")\nplt.savefig(os.path.join(output_dir, f\"train_rows\"+y_rows+\"_vs_difference.png\"))\nplt.show()\nprint(\"\\n\")\n\npolyline = np.linspace(1400, 4000, 100)  \nplt.figure(figsize=(6,3))\nnum_models = len(rows_models) // 2\nfor imod in range(num_models):\n    plt.plot(polyline, rows_models[2*imod](polyline), c='blue',alpha=0.6)\n    plt.plot(polyline, rows_models[2*imod+1](polyline), c='red',alpha=0.6)\nplt.xlabel(\"Average Surface Velocity (m/s)\")\nplt.ylabel(\"Median of Rows\")\nplt.title(\"Fits of Row-Ranges Medians vs. Surface Velocity\")\nplt.savefig(os.path.join(output_dir, f\"Average Surface Velocity.png\"))\nplt.show()\n\n# What/why are the blue lines in the 1029 and 3049 median vs surface velocity plots?\n# Find the samples in these lines\ntrainblue = traindf[traindf[\"diff_clr\"] == 'blue']\n\nprint(\"\\n\\n  Look for 'blue' samples that have Rows Medians equal to the y=0 Average.\")\nprint(\"  - List the counts of Velocity-Map Types.\")\nprint(\"  - Check the y0Diff values: they are all 0, so vmaps are R-L symmetric.\\n\\n\")\n\nfor yrows in [\"09L\",\"09R\",\"1029\",\"3049\",\"5069\"]:\n    plt.figure(figsize=(6,2))\n    plt.hist(np.clip(trainblue[\"y_\"+yrows+\"Medi\"] - trainblue[\"y_y0Ave\"],-800,800),\n             log=True, bins=160)\n    plt.xlim(-500,500)\n    plt.xlabel(\"Rows \"+yrows+\" Median  -  y=0 Average\")\n    plt.savefig(os.path.join(output_dir, f\"train_hist_{yrows}_difference.png\"))\n    plt.show()\n\n    matchdf = trainblue[np.abs(trainblue[\"y_\"+yrows+\"Medi\"] - trainblue[\"y_y0Ave\"]) < 0.0001]\n    print(matchdf[\"veltype\"].value_counts())\n    print(matchdf[\"y_y0Diff\"].value_counts())\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _cfg.py\n\n#Configuration file\nfrom types import SimpleNamespace\nimport torch\n\ncfg= SimpleNamespace()\ncfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncfg.local_rank = 0\ncfg.seed = 123\ncfg.subsample = None\n\ncfg.backbone = \"convnext_small.fb_in22k_ft_in1k\"\ncfg.ema = True\ncfg.ema_decay = 0.99\n\ncfg.epochs = 1\ncfg.batch_size = 16\ncfg.batch_size_val = 16\n\ncfg.early_stopping = {\"patience\": 3, \"streak\": 0}\ncfg.logging_steps = 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T02:10:33.787530Z","iopub.status.idle":"2025-07-02T02:10:33.787831Z","shell.execute_reply.started":"2025-07-02T02:10:33.787699Z","shell.execute_reply":"2025-07-02T02:10:33.787715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _dataset.py\n# Dataset\n\nimport os\nimport glob\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(\n        self, \n        cfg,\n        mode = \"train\", \n    ):\n        self.cfg = cfg\n        self.mode = mode\n        \n        self.data, self.labels, self.records = self.load_metadata()\n\n    def load_metadata(self, ):\n\n        # Select rows\n        df= pd.read_csv(\"/kaggle/input/openfwi-preprocessed-72x72/folds.csv\")\n        if self.cfg.subsample is not None:\n            df= df.groupby([\"dataset\", \"fold\"]).head(self.cfg.subsample)\n\n        if self.mode == \"train\":\n            df= df[df[\"fold\"] != 0]\n        else:\n            df= df[df[\"fold\"] == 0]\n\n        \n        data = []\n        labels = []\n        records = []\n        mmap_mode = \"r\"\n\n        for idx, row in tqdm(df.iterrows(), total=len(df), disable=self.cfg.local_rank != 0):\n            row= row.to_dict()\n\n            # Hacky way to get exact file name\n            p1 = os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", row[\"data_fpath\"])\n            p2 = os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n            p3 = os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", row[\"data_fpath\"])\n            p4 = os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n            farr= glob.glob(p1) + glob.glob(p2) + glob.glob(p3) + glob.glob(p4)\n        \n            # Map to lbl fpath\n            farr= farr[0]\n            flbl= farr.replace('seis', 'vel').replace('data', 'model')\n            \n            # Load\n            arr= np.load(farr, mmap_mode=mmap_mode)\n            lbl= np.load(flbl, mmap_mode=mmap_mode)\n\n            # Append\n            data.append(arr)\n            labels.append(lbl)\n            records.append(row[\"dataset\"])\n\n        return data, labels, records\n\n    def __getitem__(self, idx):\n        row_idx= idx // 500\n        col_idx= idx % 500\n\n        d= self.records[row_idx]\n        x= self.data[row_idx][col_idx, ...]\n        y= self.labels[row_idx][col_idx, ...]\n\n        # Augs \n        if self.mode == \"train\":\n            \n            # Temporal flip\n            if np.random.random() < 0.5:\n                x= x[::-1, :, ::-1]\n                y= y[..., ::-1]\n\n        x= x.copy()\n        y= y.copy()\n        \n        return x, y\n\n    def __len__(self, ):\n        return len(self.records) * 500","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:03:11.649134Z","iopub.execute_input":"2025-07-01T18:03:11.649685Z","iopub.status.idle":"2025-07-01T18:03:11.655118Z","shell.execute_reply.started":"2025-07-01T18:03:11.649659Z","shell.execute_reply":"2025-07-01T18:03:11.654444Z"}},"outputs":[{"name":"stdout","text":"Writing _dataset.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile _utils.py\n\n#Date and Time Format\nimport datetime\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:03:17.597467Z","iopub.execute_input":"2025-07-01T18:03:17.597903Z","iopub.status.idle":"2025-07-01T18:03:17.602608Z","shell.execute_reply.started":"2025-07-01T18:03:17.597883Z","shell.execute_reply":"2025-07-01T18:03:17.601941Z"}},"outputs":[{"name":"stdout","text":"Writing _utils.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile _model.py\n\nfrom copy import deepcopy\nfrom types import MethodType\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport timm\nfrom timm.models.convnext import ConvNeXtBlock\n\nfrom monai.networks.blocks import UpSample, SubpixelUpsample\n\n####################\n## EMA + Ensemble ##\n####################\n\nclass ModelEMA(nn.Module):\n    def __init__(self, model, decay=0.99, device=None):\n        super().__init__()\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super().__init__()\n        self.models = nn.ModuleList(models).eval()\n\n    def forward(self, x):\n        output = None\n        \n        for m in self.models:\n            logits= m(x)\n            \n            if output is None:\n                output = logits\n            else:\n                output += logits\n                \n        output /= len(self.models)\n        return output\n        \n\n#############\n## Decoder ##\n#############\n\nclass ConvBnAct2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding: int = 0,\n        stride: int = 1,\n        norm_layer: nn.Module = nn.Identity,\n        act_layer: nn.Module = nn.ReLU,\n    ):\n        super().__init__()\n\n        self.conv= nn.Conv2d(\n            in_channels, \n            out_channels,\n            kernel_size,\n            stride=stride, \n            padding=padding, \n            bias=False,\n        )\n        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n        self.act= act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.act(x)\n        return x\n\n\nclass SCSEModule2d(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.Tanh(),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(\n            nn.Conv2d(in_channels, 1, 1), \n            nn.Sigmoid(),\n            )\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\nclass Attention2d(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \"scse\":\n            self.attention = SCSEModule2d(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n\nclass DecoderBlock2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n        scale_factor: int = 2,\n    ):\n        super().__init__()\n\n        # Upsample block\n        if upsample_mode == \"pixelshuffle\":\n            self.upsample= SubpixelUpsample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                scale_factor= scale_factor,\n            )\n        else:\n            self.upsample = UpSample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                out_channels= in_channels,\n                scale_factor= scale_factor,\n                mode= upsample_mode,\n            )\n\n        if intermediate_conv:\n            k= 3\n            c= skip_channels if skip_channels != 0 else in_channels\n            self.intermediate_conv = nn.Sequential(\n                ConvBnAct2d(c, c, k, k//2),\n                ConvBnAct2d(c, c, k, k//2),\n                )\n        else:\n            self.intermediate_conv= None\n\n        self.attention1 = Attention2d(\n            name= attention_type, \n            in_channels= in_channels + skip_channels,\n            )\n\n        self.conv1 = ConvBnAct2d(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n\n        self.conv2 = ConvBnAct2d(\n            out_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n        self.attention2 = Attention2d(\n            name= attention_type, \n            in_channels= out_channels,\n            )\n        self.dropout = nn.Dropout2d(p=0.2)  #improvement # 3\n\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n\n        if self.intermediate_conv is not None:\n            if skip is not None:\n                skip = self.intermediate_conv(skip)\n            else:\n                x = self.intermediate_conv(x)\n\n        if skip is not None:\n            # print(x.shape, skip.shape)\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.dropout(x)   #Improvement # 3\n        x = self.attention2(x)\n        return x\n\n\nclass UnetDecoder2d(nn.Module):\n    \"\"\"\n    Unet decoder.\n    Source: https://arxiv.org/abs/1505.04597\n    \"\"\"\n    def __init__(\n        self,\n        encoder_channels: tuple[int],\n        skip_channels: tuple[int] = None,\n        decoder_channels: tuple = (256, 128, 64, 32),\n        scale_factors: tuple = (2,2,2,2),\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n    ):\n        super().__init__()\n        \n        if len(encoder_channels) == 4:\n            decoder_channels= decoder_channels[1:]\n        self.decoder_channels= decoder_channels\n        \n        if skip_channels is None:\n            skip_channels= list(encoder_channels[1:]) + [0]\n\n        # Build decoder blocks\n        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n        self.blocks = nn.ModuleList()\n\n        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n            # print(i, ic, sc, dc)\n            self.blocks.append(\n                DecoderBlock2d(\n                    ic, sc, dc, \n                    norm_layer= norm_layer,\n                    attention_type= attention_type,\n                    intermediate_conv= intermediate_conv,\n                    upsample_mode= upsample_mode,\n                    scale_factor= scale_factors[i],\n                    )\n            )\n\n    def forward(self, feats: list[torch.Tensor]):\n        res= [feats[0]]\n        feats= feats[1:]\n\n        # Decoder blocks\n        for i, b in enumerate(self.blocks):\n            skip= feats[i] if i < len(feats) else None\n            res.append(\n                b(res[-1], skip=skip),\n                )\n            \n        return res\n\nclass SegmentationHead2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        scale_factor: tuple[int] = (2,2),\n        kernel_size: int = 3,\n        mode: str = \"nontrainable\",\n    ):\n        super().__init__()\n        self.conv= nn.Conv2d(\n            in_channels, out_channels, kernel_size= kernel_size,\n            padding= kernel_size//2\n        )\n        self.upsample = UpSample(\n            spatial_dims= 2,\n            in_channels= out_channels,\n            out_channels= out_channels,\n            scale_factor= scale_factor,\n            mode= mode,\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.upsample(x)\n        return x\n        \n\n#############\n## Encoder ##\n#############\n\ndef _convnext_block_forward(self, x):\n    shortcut = x\n    x = self.conv_dw(x)\n\n    if self.use_conv_mlp:\n        x = self.norm(x)\n        x = self.mlp(x)\n    else:\n        x = self.norm(x)\n        x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n        x = self.mlp(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.contiguous()\n\n    if self.gamma is not None:\n        x = x * self.gamma.reshape(1, -1, 1, 1)\n\n    x = self.drop_path(x) + self.shortcut(shortcut)\n    return x\n\n\nclass Net(nn.Module):\n    def __init__(\n        self,\n        backbone: str,\n        pretrained: bool = True,\n    ):\n        super().__init__()\n        \n        # Encoder\n        self.backbone= timm.create_model(\n            backbone,\n            in_chans= 5,\n            pretrained= pretrained,\n            features_only= True,\n            drop_path_rate=0.0,\n            )\n        ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n\n        # Decoder\n        self.decoder= UnetDecoder2d(\n            encoder_channels= ecs,\n        )\n\n        self.seg_head= SegmentationHead2d(\n            in_channels= self.decoder.decoder_channels[-1],\n            out_channels= 1,\n            scale_factor= 1,\n        )\n        \n        self._update_stem(backbone)\n        \n        self.replace_activations(self.backbone, log=True)\n        self.replace_norms(self.backbone, log=True)\n        self.replace_forwards(self.backbone, log=True)\n\n    def _update_stem(self, backbone):\n        if backbone.startswith(\"convnext\"):\n\n            # Update stride\n            self.backbone.stem_0.stride = (4, 1)\n            self.backbone.stem_0.padding = (0, 2)\n\n            # Duplicate stem layer (to downsample height)\n            with torch.no_grad():\n                w = self.backbone.stem_0.weight\n                new_conv= nn.Conv2d(w.shape[0], w.shape[0], kernel_size=(4, 4), stride=(4, 1), padding=(0, 1))\n                new_conv.weight.copy_(w.repeat(1, (128//w.shape[1])+1, 1, 1)[:, :new_conv.weight.shape[1], :, :])\n                new_conv.bias.copy_(self.backbone.stem_0.bias)\n\n            self.backbone.stem_0= nn.Sequential(\n                nn.ReflectionPad2d((1,1,80,80)),\n                self.backbone.stem_0,\n                new_conv,\n            )\n\n        else:\n            raise ValueError(\"Custom striding not implemented.\")\n        pass\n\n    def replace_activations(self, module, log=False):\n        if log:\n            print(f\"Replacing all activations with GELU...\")\n        \n        # Apply activations\n        for name, child in module.named_children():\n            if isinstance(child, (\n                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid, \n                nn.Tanh, nn.Softmax, nn.Hardtanh, nn.ELU, \n                nn.SELU, nn.PReLU, nn.CELU, nn.GELU, nn.SiLU,\n            )):\n                setattr(module, name, nn.GELU())\n            else:\n                self.replace_activations(child)\n\n    def replace_norms(self, mod, log=False):\n        if log:\n            print(f\"Replacing all norms with InstanceNorm...\")\n            \n        for name, c in mod.named_children():\n\n            # Get feature size\n            n_feats= None\n            if isinstance(c, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n                n_feats= c.num_features\n            elif isinstance(c, (nn.GroupNorm,)):\n                n_feats= c.num_channels\n            elif isinstance(c, (nn.LayerNorm,)):\n                n_feats= c.normalized_shape[0]\n\n            if n_feats is not None:\n                new = nn.InstanceNorm2d(\n                    n_feats,\n                    affine=True,\n                    )\n                setattr(mod, name, new)\n            else:\n                self.replace_norms(c)\n\n    def replace_forwards(self, mod, log=False):\n        if log:\n            print(f\"Replacing forward functions...\")\n            \n        for name, c in mod.named_children():\n            if isinstance(c, ConvNeXtBlock):\n                c.forward = MethodType(_convnext_block_forward, c)\n            else:\n                self.replace_forwards(c)\n\n        \n    def proc_flip(self, x_in):\n        x_in= torch.flip(x_in, dims=[-3, -1])\n        x= self.backbone(x_in)\n        x= x[::-1]\n\n        # Decoder\n        x= self.decoder(x)\n        x_seg= self.seg_head(x[-1])\n        x_seg= x_seg[..., 1:-1, 1:-1]\n        x_seg= torch.flip(x_seg, dims=[-1])\n        x_seg= x_seg * 1500 + 3000\n        return x_seg\n\n    def forward(self, batch):\n        x= batch\n\n        # Encoder\n        x_in = x\n        x= self.backbone(x)\n        # print([_.shape for _ in x])\n        x= x[::-1]\n\n        # Decoder\n        x= self.decoder(x)\n        # print([_.shape for _ in x])\n        x_seg= self.seg_head(x[-1])\n        x_seg= x_seg[..., 1:-1, 1:-1]\n        x_seg= x_seg * 1500 + 3000\n    \n        if self.training:\n            return x_seg\n        else:\n            p1 = self.proc_flip(x_in)\n            x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n            return x_seg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T18:05:07.886713Z","iopub.execute_input":"2025-07-01T18:05:07.887028Z","iopub.status.idle":"2025-07-01T18:05:07.896801Z","shell.execute_reply.started":"2025-07-01T18:05:07.887003Z","shell.execute_reply":"2025-07-01T18:05:07.896109Z"}},"outputs":[{"name":"stdout","text":"Overwriting _model.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Train the model\n\nimport os\nimport time \nimport random\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.amp import autocast, GradScaler\n\nimport torch.distributed as dist\nfrom torch.utils.data import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom _cfg import cfg\nfrom _dataset import CustomDataset\nfrom _model import ModelEMA, Net, EnsembleModel\nfrom _utils import format_time\nimport glob\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\ndef setup(rank, world_size):\n    if not dist.is_initialized():\n        os.environ.setdefault(\"MASTER_ADDR\", \"localhost\")\n        os.environ.setdefault(\"MASTER_PORT\", \"12355\")\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\ndef cleanup():\n    dist.barrier()\n    dist.destroy_process_group()\n    return\n\ndef main(cfg):\n\n    # ========== Datasets / Dataloaders ==========\n    if cfg.local_rank == 0:\n        print(\"=\"*25)\n        print(\"Loading data..\")\n    train_ds = CustomDataset(cfg=cfg, mode=\"train\")\n    sampler= DistributedSampler(\n        train_ds, \n        num_replicas=cfg.world_size, \n        rank=cfg.local_rank,\n    )\n    train_dl = torch.utils.data.DataLoader(\n        train_ds, \n        sampler= sampler,\n        batch_size= cfg.batch_size, \n        num_workers= 4,\n    )\n    \n    valid_ds = CustomDataset(cfg=cfg, mode=\"valid\")\n    sampler= DistributedSampler(\n        valid_ds, \n        num_replicas=cfg.world_size, \n        rank=cfg.local_rank,\n    )\n    valid_dl = torch.utils.data.DataLoader(\n        valid_ds, \n        sampler= sampler,\n        batch_size= cfg.batch_size_val, \n        num_workers= 4,\n    )\n\n    # ========== Model / Optim ==========\n    # Load pretrained models\n    models = []\n    for f in sorted(glob.glob(\"/kaggle/input/simple-further-finetuned-bartley-open-models/*.pth\")):\n        print(\"Loading: \", f)\n        m = Net(\n            backbone=\"convnext_small.fb_in22k_ft_in1k\",\n            pretrained=False,\n        )\n        state_dict= torch.load(f, map_location=cfg.device, weights_only=True)\n        state_dict= {k.removeprefix(\"_orig_mod.\"):v for k,v in state_dict.items()} # Remove torch.compile() prefix\n\n        m.load_state_dict(state_dict)\n        models.append(m)    \n      # Combine\n    model = EnsembleModel(models)\n    model= model.to(cfg.local_rank)\n    if cfg.ema:\n        if cfg.local_rank == 0:\n            print(\"Initializing EMA model..\")\n        ema_model = ModelEMA(\n            model, \n            decay=cfg.ema_decay, \n            device=cfg.local_rank,\n        )\n    else:\n        ema_model = None\n    model= DistributedDataParallel(\n        model, \n        device_ids=[cfg.local_rank], \n        )\n    \n    #criterion = nn.L1Loss()\n    criterion = nn.SmoothL1Loss(beta=50.0) # improvement #2\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs) # Improvement # 4\n\n    scaler = GradScaler()\n\n\n    # ========== Training ==========\n    if cfg.local_rank == 0:\n        print(\"=\"*25)\n        print(\"Give me warp {}, Mr. Mamun.\".format(cfg.world_size))\n        print(\"=\"*25)\n    \n    best_loss= 1_000_000\n    val_loss= 1_000_000\n\n    for epoch in range(0, cfg.epochs+1):\n        if epoch != 0:\n            tstart= time.time()\n            train_dl.sampler.set_epoch(epoch)\n    \n            # Train loop\n            print(f\"Training Loop Epoch #:{epoch}\")\n            model.train()\n            total_loss = []\n            for i, (x, y) in enumerate(train_dl):\n                x = x.to(cfg.local_rank)\n                y = y.to(cfg.local_rank)\n        \n                with autocast(cfg.device.type):\n                    logits = model(x)\n                    \n                loss = criterion(logits, y)\n        \n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n        \n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n        \n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n    \n                total_loss.append(loss.item())\n                \n                if ema_model is not None:\n                    ema_model.update(model)\n                    \n                if cfg.local_rank == 0 and (len(total_loss) >= cfg.logging_steps or i == 0):\n                    train_loss = np.mean(total_loss)\n                    total_loss = []\n                    print(\"Epoch {}:     Train MAE: {:.2f}     Val MAE: {:.2f}     Time: {}     Step: {}/{}\".format(\n                        epoch, \n                        train_loss,\n                        val_loss,\n                        format_time(time.time() - tstart),\n                        i+1, \n                        len(train_dl)+1, \n                    ))\n    \n        # ========== Valid ==========\n        model.eval()\n        val_logits = []\n        val_targets = []\n        with torch.no_grad():\n            for x, y in tqdm(valid_dl, disable=cfg.local_rank != 0):\n                x = x.to(cfg.local_rank)\n                y = y.to(cfg.local_rank)\n    \n                with autocast(cfg.device.type):\n                    if ema_model is not None:\n                        out = ema_model.module(x)\n                    else:\n                        out = model(x)\n\n                val_logits.append(out.cpu())\n                val_targets.append(y.cpu())\n\n            val_logits= torch.cat(val_logits, dim=0)\n            val_targets= torch.cat(val_targets, dim=0)\n                \n            loss = criterion(val_logits, val_targets).item()\n\n        # Gather loss\n        v = torch.tensor([loss], device=cfg.local_rank)\n        torch.distributed.all_reduce(v, op=dist.ReduceOp.SUM)\n        val_loss = (v[0] / cfg.world_size).item()\n\n        # Improvement # 4 Cosine Annealing Scheduler\n        scheduler.step()\n\n        # ========== Weights / Early stopping ==========\n        stop_train = torch.tensor([0], device=cfg.local_rank)\n        if cfg.local_rank == 0:\n            es= cfg.early_stopping\n            if val_loss < best_loss:\n                print(\"New best: {:.2f} -> {:.2f}\".format(best_loss, val_loss))\n                print(\"Saved weights..\")\n                best_loss = val_loss\n                if ema_model is not None:\n                    torch.save(ema_model.module.state_dict(), f'best_model_{cfg.seed}.pt')\n                else:\n                    torch.save(model.state_dict(), f'best_model_{cfg.seed}.pt')\n        \n                es[\"streak\"] = 0\n            else:\n                es= cfg.early_stopping\n                es[\"streak\"] += 1\n                if es[\"streak\"] > es[\"patience\"]:\n                    print(\"Ending training (early_stopping).\")\n                    stop_train = torch.tensor([1], device=cfg.local_rank)\n        \n        # Exits training on all ranks\n        dist.broadcast(stop_train, src=0)\n        if stop_train.item() == 1:\n            return\n\n    return\n    \n\n\nif __name__ == \"__main__\":\n\n    # GPU Specs\n    rank = int(os.environ.get(\"RANK\", 0))  # Default rank 0\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))  # Default world size 1\n    _, total = torch.cuda.mem_get_info(device=rank)\n\n    # Init\n    setup(rank, world_size)\n    time.sleep(rank)\n    print(f\"Rank: {rank}, World size: {world_size}, GPU memory: {total / 1024**3:.2f}GB\", flush=True)\n    time.sleep(world_size - rank)\n\n    # Seed\n    set_seed(cfg.seed+rank)\n\n    # Run\n    cfg.local_rank= rank\n    cfg.world_size= world_size\n    main(cfg)\n    cleanup()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T02:04:35.246503Z","iopub.execute_input":"2025-07-02T02:04:35.247038Z","iopub.status.idle":"2025-07-02T02:04:40.176285Z","shell.execute_reply.started":"2025-07-02T02:04:35.246984Z","shell.execute_reply":"2025-07-02T02:04:40.175274Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/243369675.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_cfg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelEMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnsembleModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_cfg'"],"ename":"ModuleNotFoundError","evalue":"No module named '_cfg'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"#%%writefile _model.py\n# modified model, uncomment the line above and generate the model and then train on sample data.\n'''\n1. Final Output: Apply tanh() + Output Scaling\n2. Loss Function: Use Smooth L1 (Huber) Loss\n3. Dropout in Decoder\n4. Cosine Annealing Scheduler\n5. (Optional) Use GroupNorm Instead of InstanceNorm\n6. (Future) Add Vertical Flip in proc_flip()\n'''\n\nfrom copy import deepcopy\nfrom types import MethodType\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport timm\nfrom timm.models.convnext import ConvNeXtBlock\n\nfrom monai.networks.blocks import UpSample, SubpixelUpsample\n\n####################\n## EMA + Ensemble ##\n####################\n\nclass ModelEMA(nn.Module):\n    def __init__(self, model, decay=0.99, device=None):\n        super().__init__()\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super().__init__()\n        self.models = nn.ModuleList(models).eval()\n\n    def forward(self, x):\n        output = None\n        \n        for m in self.models:\n            logits= m(x)\n            \n            if output is None:\n                output = logits\n            else:\n                output += logits\n                \n        output /= len(self.models)\n        return output\n        \n\n#############\n## Decoder ##\n#############\n\nclass ConvBnAct2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding: int = 0,\n        stride: int = 1,\n        norm_layer: nn.Module = nn.Identity,\n        act_layer: nn.Module = nn.ReLU,\n    ):\n        super().__init__()\n\n        self.conv= nn.Conv2d(\n            in_channels, \n            out_channels,\n            kernel_size,\n            stride=stride, \n            padding=padding, \n            bias=False,\n        )\n        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n        self.act= act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.act(x)\n        return x\n\n\nclass SCSEModule2d(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.Tanh(),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(\n            nn.Conv2d(in_channels, 1, 1), \n            nn.Sigmoid(),\n            )\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\nclass Attention2d(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \"scse\":\n            self.attention = SCSEModule2d(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n\nclass DecoderBlock2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n        scale_factor: int = 2,\n    ):\n        super().__init__()\n\n        # Upsample block\n        if upsample_mode == \"pixelshuffle\":\n            self.upsample= SubpixelUpsample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                scale_factor= scale_factor,\n            )\n        else:\n            self.upsample = UpSample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                out_channels= in_channels,\n                scale_factor= scale_factor,\n                mode= upsample_mode,\n            )\n\n        if intermediate_conv:\n            k= 3\n            c= skip_channels if skip_channels != 0 else in_channels\n            self.intermediate_conv = nn.Sequential(\n                ConvBnAct2d(c, c, k, k//2),\n                ConvBnAct2d(c, c, k, k//2),\n                )\n        else:\n            self.intermediate_conv= None\n\n        self.attention1 = Attention2d(\n            name= attention_type, \n            in_channels= in_channels + skip_channels,\n            )\n\n        self.conv1 = ConvBnAct2d(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n\n        self.conv2 = ConvBnAct2d(\n            out_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n        self.attention2 = Attention2d(\n            name= attention_type, \n            in_channels= out_channels,\n            )\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n\n        if self.intermediate_conv is not None:\n            if skip is not None:\n                skip = self.intermediate_conv(skip)\n            else:\n                x = self.intermediate_conv(x)\n\n        if skip is not None:\n            # print(x.shape, skip.shape)\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\n\nclass UnetDecoder2d(nn.Module):\n    \"\"\"\n    Unet decoder.\n    Source: https://arxiv.org/abs/1505.04597\n    \"\"\"\n    def __init__(\n        self,\n        encoder_channels: tuple[int],\n        skip_channels: tuple[int] = None,\n        decoder_channels: tuple = (256, 128, 64, 32),\n        scale_factors: tuple = (2,2,2,2),\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n    ):\n        super().__init__()\n        \n        if len(encoder_channels) == 4:\n            decoder_channels= decoder_channels[1:]\n        self.decoder_channels= decoder_channels\n        \n        if skip_channels is None:\n            skip_channels= list(encoder_channels[1:]) + [0]\n\n        # Build decoder blocks\n        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n        self.blocks = nn.ModuleList()\n\n        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n            # print(i, ic, sc, dc)\n            self.blocks.append(\n                DecoderBlock2d(\n                    ic, sc, dc, \n                    norm_layer= norm_layer,\n                    attention_type= attention_type,\n                    intermediate_conv= intermediate_conv,\n                    upsample_mode= upsample_mode,\n                    scale_factor= scale_factors[i],\n                    )\n            )\n\n    def forward(self, feats: list[torch.Tensor]):\n        res= [feats[0]]\n        feats= feats[1:]\n\n        # Decoder blocks\n        for i, b in enumerate(self.blocks):\n            skip= feats[i] if i < len(feats) else None\n            res.append(\n                b(res[-1], skip=skip),\n                )\n            \n        return res\n\nclass SegmentationHead2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        scale_factor: tuple[int] = (2,2),\n        kernel_size: int = 3,\n        mode: str = \"nontrainable\",\n    ):\n        super().__init__()\n        self.conv= nn.Conv2d(\n            in_channels, out_channels, kernel_size= kernel_size,\n            padding= kernel_size//2\n        )\n        self.upsample = UpSample(\n            spatial_dims= 2,\n            in_channels= out_channels,\n            out_channels= out_channels,\n            scale_factor= scale_factor,\n            mode= mode,\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.upsample(x)\n        return x\n        \n\n#############\n## Encoder ##\n#############\n\ndef _convnext_block_forward(self, x):\n    shortcut = x\n    x = self.conv_dw(x)\n\n    if self.use_conv_mlp:\n        x = self.norm(x)\n        x = self.mlp(x)\n    else:\n        x = self.norm(x)\n        x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n        x = self.mlp(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.contiguous()\n\n    if self.gamma is not None:\n        x = x * self.gamma.reshape(1, -1, 1, 1)\n\n    x = self.drop_path(x) + self.shortcut(shortcut)\n    return x\n\n\nclass Net(nn.Module):\n    def __init__(\n        self,\n        backbone: str,\n        pretrained: bool = True,\n    ):\n        super().__init__()\n        \n        # Encoder\n        self.backbone= timm.create_model(\n            backbone,\n            in_chans= 5,\n            pretrained= pretrained,\n            features_only= True,\n            drop_path_rate=0.0,\n            )\n        ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n\n        # Decoder\n        self.decoder= UnetDecoder2d(\n            encoder_channels= ecs,\n        )\n\n        self.seg_head= SegmentationHead2d(\n            in_channels= self.decoder.decoder_channels[-1],\n            out_channels= 1,\n            scale_factor= 1,\n        )\n        \n        self._update_stem(backbone)\n        \n        self.replace_activations(self.backbone, log=True)\n        self.replace_norms(self.backbone, log=True)\n        self.replace_forwards(self.backbone, log=True)\n\n    def _update_stem(self, backbone):\n        if backbone.startswith(\"convnext\"):\n\n            # Update stride\n            self.backbone.stem_0.stride = (4, 1)\n            self.backbone.stem_0.padding = (0, 2)\n\n            # Duplicate stem layer (to downsample height)\n            with torch.no_grad():\n                w = self.backbone.stem_0.weight\n                new_conv= nn.Conv2d(w.shape[0], w.shape[0], kernel_size=(4, 4), stride=(4, 1), padding=(0, 1))\n                new_conv.weight.copy_(w.repeat(1, (128//w.shape[1])+1, 1, 1)[:, :new_conv.weight.shape[1], :, :])\n                new_conv.bias.copy_(self.backbone.stem_0.bias)\n\n            self.backbone.stem_0= nn.Sequential(\n                nn.ReflectionPad2d((1,1,80,80)),\n                self.backbone.stem_0,\n                new_conv,\n            )\n\n        else:\n            raise ValueError(\"Custom striding not implemented.\")\n        pass\n\n    def replace_activations(self, module, log=False):\n        if log:\n            print(f\"Replacing all activations with GELU...\")\n        \n        # Apply activations\n        for name, child in module.named_children():\n            if isinstance(child, (\n                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid, \n                nn.Tanh, nn.Softmax, nn.Hardtanh, nn.ELU, \n                nn.SELU, nn.PReLU, nn.CELU, nn.GELU, nn.SiLU,\n            )):\n                setattr(module, name, nn.GELU())\n            else:\n                self.replace_activations(child)\n\n    def replace_norms(self, mod, log=False):\n        if log:\n            print(f\"Replacing all norms with InstanceNorm...\")\n            \n        for name, c in mod.named_children():\n\n            # Get feature size\n            n_feats= None\n            if isinstance(c, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n                n_feats= c.num_features\n            elif isinstance(c, (nn.GroupNorm,)):\n                n_feats= c.num_channels\n            elif isinstance(c, (nn.LayerNorm,)):\n                n_feats= c.normalized_shape[0]\n\n            if n_feats is not None:\n                new = nn.GroupNorm( #improvement 5: instead of InstanceNorm2d using GroupNorm\n                    num_groups=8,\n                    num_channels=n_feats,\n                )                                \n                # new = nn.InstanceNorm2d(\n                #     n_feats,\n                #     affine=True,\n                #     )\n                setattr(mod, name, new)\n            else:\n                self.replace_norms(c)\n\n    def replace_forwards(self, mod, log=False):\n        if log:\n            print(f\"Replacing forward functions...\")\n            \n        for name, c in mod.named_children():\n            if isinstance(c, ConvNeXtBlock):\n                c.forward = MethodType(_convnext_block_forward, c)\n            else:\n                self.replace_forwards(c)\n\n        \n    def proc_flip(self, x_in):\n        x_in= torch.flip(x_in, dims=[-3, -1])\n        x= self.backbone(x_in)\n        x= x[::-1]\n\n        # Decoder\n        x= self.decoder(x)\n        x_seg= self.seg_head(x[-1])\n        x_seg= x_seg[..., 1:-1, 1:-1]\n        x_seg= torch.flip(x_seg, dims=[-1])\n        x_seg= x_seg * 1500 + 3000\n        return x_seg\n\n    def forward(self, batch):\n        x= batch\n\n        # Encoder\n        x_in = x\n        x= self.backbone(x)\n        # print([_.shape for _ in x])\n        x= x[::-1]\n\n        # Decoder\n        x= self.decoder(x)\n        # print([_.shape for _ in x])\n        x_seg= self.seg_head(x[-1])\n        x_seg = x_seg[..., 1:-1, 1:-1]    # following two lines- #1\n        x_seg = torch.tanh(x_seg)         # Bound output to [-1, 1]\n        x_seg = x_seg * 1500 + 3000       # Final range: [1500, 4500]\n\n    \n        if self.training:\n            return x_seg\n        else:\n            p1 = self.proc_flip(x_in)\n            x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n            return x_seg\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the new model and do the model validation\nimport torch\nimport torch.nn as nn\nfrom _cfg import cfg\nfrom _model import Net, ModelEMA, EnsembleModel\nfrom _utils import format_time\nfrom torch.utils.data import DataLoader\nfrom torch import amp\nfrom _dataset import CustomDataset\n\n# Load ensemble state_dict\nstate_dict = torch.load(\"best_model_123.pt\", map_location=\"cpu\")\n\n# Extract keys from 'models.0.' only\nsubmodel_0_state_dict = {\n    k.replace(\"models.0.\", \"\"): v\n    for k, v in state_dict.items()\n    if k.startswith(\"models.0.\")\n}\n# Save as a standalone checkpoint\ntorch.save(submodel_0_state_dict, \"best_model_123_submodel0.pt\")\n\nsubmodel_1_state_dict = {\n    k.replace(\"models.1.\", \"\"): v\n    for k, v in state_dict.items()\n    if k.startswith(\"models.1.\")\n}\ntorch.save(submodel_1_state_dict, \"best_model_123_submodel1.pt\")\n\n# Initialize the same architecture (use correct backbone!)\n#model = Net(backbone='convnext_base', pretrained=False)\nmodel = Net(backbone='convnext_small', pretrained=False)\n\n# Load the submodel weights\nmodel.load_state_dict(torch.load(\"best_model_123_submodel0.pt\", map_location=\"cpu\"), strict=True)\nmodel.eval()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nvalid_ds = CustomDataset(cfg=cfg, mode=\"valid\")\nvalid_dl = torch.utils.data.DataLoader(\n    valid_ds,\n    sampler=torch.utils.data.SequentialSampler(valid_ds),\n    batch_size=cfg.batch_size_val,\n    num_workers=4, # this won't work at local machine \n)\n\ncriterion = nn.L1Loss()\nval_logits = []\nval_targets = []\n\nwith torch.no_grad():\n    for x, y in tqdm(valid_dl):\n        x = x.to(cfg.device)\n        y = y.to(cfg.device)\n        \n        with amp.autocast(device_type=cfg.device.type, enabled=True):\n            out = model(x)\n\n        val_logits.append(out.cpu())\n        val_targets.append(y.cpu())\n\nval_logits = torch.cat(val_logits, dim=0)\nval_targets = torch.cat(val_targets, dim=0)\ntotal_loss = criterion(val_logits, val_targets).item()\n\nprint(\"=\" * 25)\nprint(\"Val MAE: {:.2f}\".format(total_loss))\nprint(\"=\" * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T00:40:42.340171Z","iopub.execute_input":"2025-07-02T00:40:42.340693Z","iopub.status.idle":"2025-07-02T00:43:07.622658Z","shell.execute_reply.started":"2025-07-02T00:40:42.340665Z","shell.execute_reply":"2025-07-02T00:43:07.621717Z"}},"outputs":[{"name":"stdout","text":"Replacing all activations with GELU...\nReplacing all norms with InstanceNorm...\nReplacing forward functions...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:01<00:00, 13.84it/s]\n100%|██████████| 625/625 [02:20<00:00,  4.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"=========================\nVal MAE: 30.62\n=========================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler\nfrom tqdm import tqdm\nimport glob\nimport csv\nimport time\nfrom torch.amp import autocast\n\n# === Custom Dataset for Test ===\nclass TestDataset(Dataset):\n    def __init__(self, test_files):\n        self.test_files = test_files\n\n    def __len__(self):\n        return len(self.test_files)\n\n    def __getitem__(self, i):\n        test_file = self.test_files[i]\n        test_stem = test_file.split(\"/\")[-1].split(\".\")[0]\n        x = np.load(test_file).astype(np.float32)\n        x = torch.tensor(x)\n        return x, test_stem\n\nRUN_TEST = True\nif RUN_TEST:\n\n    submission_template = pd.read_csv(\"/kaggle/input/waveform-inversion/sample_submission.csv\")\n\n    test_files = sorted(glob.glob(\"/kaggle/input/waveform-inversion/test/*.npy\"))\n    test_ds = TestDataset(test_files)\n    test_dl = DataLoader(\n        test_ds,\n        sampler=SequentialSampler(test_ds),\n        batch_size=cfg.batch_size_val,\n        num_workers=4,\n    )\n\n    x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n    fieldnames = [\"oid_ypos\"] + x_cols\n    row_count = 0\n    t0 = time.time()\n\n    with open(\"submission.csv\", \"wt\", newline=\"\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        with torch.no_grad():\n            for inputs, oids in tqdm(test_dl):\n                inputs = inputs.to(cfg.device)\n\n                with autocast(device_type=cfg.device.type):  # 'cuda' or 'cpu'\n                    # Predit with the model\n                    outputs = model(inputs)\n\n                outputs = outputs.cpu().numpy()  # shape: [B, 1, 70, 69]\n\n                for batch_idx, oid in enumerate(oids):\n                    y_pred = outputs[batch_idx, 0]  # shape [70, 69]\n\n                    for y_pos in range(y_pred.shape[0]):\n                        row = {f\"x_{i}\": float(y_pred[y_pos][i]) for i in range(1, 70, 2)}\n                        row[\"oid_ypos\"] = f\"{oid}_y_{y_pos}\"\n                        writer.writerow(row)\n                        row_count += 1\n\n                        if row_count % 100000 == 0:\n                            csvfile.flush()\n\n    print(\"Saved predictions to submission.csv\")\n    print(f\"Inference Time: {format_time(time.time() - t0)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T00:54:13.940706Z","iopub.execute_input":"2025-07-02T00:54:13.941066Z","iopub.status.idle":"2025-07-02T01:14:58.142382Z","shell.execute_reply.started":"2025-07-02T00:54:13.941034Z","shell.execute_reply":"2025-07-02T01:14:58.141310Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4114/4114 [20:25<00:00,  3.36it/s]","output_type":"stream"},{"name":"stdout","text":"Saved predictions to submission.csv\nInference Time: 0:20:26\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# WebApp - uses stremlit and publish locally.\n\nimport streamlit as st\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom _model import Net  # Make sure _model.py is in the same directory\nimport os\nimport torch._classes  # Prevent Streamlit torch watcher crash\nos.environ[\"STREAMLIT_WATCHER_TYPE\"] = \"none\"\n\n# Title\nst.set_page_config(layout=\"wide\")\nst.title(\"Seismic Velocity Map Prediction App\")\n\n# Sidebar\nst.sidebar.header(\"Upload Files\")\nuploaded_file = st.sidebar.file_uploader(\"Upload Seismic Waveform (.npy)\", type=[\"npy\"])\nvelocity_file = st.sidebar.file_uploader(\"(Optional) Upload Ground Truth Velocity Map (.npy)\", type=[\"npy\"])\nmodel_type = st.sidebar.selectbox(\"Select Model\", [\"VelocityGAN\", \"InversionNet\", \"ConvNeXt U-Net\"])\n\n# Load ConvNeXt U-Net model\n@st.cache_resource\ndef load_convnext_unet_model(path=\"best_model_1234_submodel0.pt\"):\n    model = Net(backbone='convnext_small', pretrained=False)\n    state_dict = torch.load(path, map_location='cpu')\n    model.load_state_dict(state_dict)\n    model = model.eval().float()\n    return model\n\n# Dummy GAN/InversionNet loaders (replace with real)\ndef load_velocitygan_model():\n    st.warning(\"VelocityGAN model loading not implemented.\")\n    return lambda x: x\n\ndef load_inversionnet_model():\n    st.warning(\"InversionNet model loading not implemented.\")\n    return lambda x: x\n\n# Inference with ConvNeXt\ndef predict_with_convnext(model, input_waveform):\n    with torch.no_grad():\n        x = torch.tensor(input_waveform).float()\n        if x.dim() == 2:\n            x = x.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n        elif x.dim() == 3:\n            x = x.unsqueeze(0)  # [1, C, H, W]\n        output = model(x).squeeze().numpy()  # [H, W] or [5, H, W]\n    return output\n\n# Main Prediction\nif st.button(\"Predict\") and uploaded_file is not None:\n    waveform = np.load(uploaded_file)\n\n    if model_type == \"ConvNeXt U-Net\":\n        model = load_convnext_unet_model()\n        prediction = predict_with_convnext(model, waveform)\n    elif model_type == \"VelocityGAN\":\n        model = load_velocitygan_model()\n        prediction = model(waveform)\n    else:\n        model = load_inversionnet_model()\n        prediction = model(waveform)\n\n    # Normalize prediction shape\n    if prediction.ndim == 3:\n        pred_img = prediction.mean(axis=0)  # (H, W)\n    elif prediction.ndim == 2:\n        pred_img = prediction\n    else:\n        raise ValueError(f\"Unexpected prediction shape: {prediction.shape}\")\n\n    # Upsample to higher resolution for display\n    pred_tensor = torch.tensor(pred_img).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n    pred_hr = F.interpolate(pred_tensor, scale_factor=2, mode='bilinear', align_corners=False)\n    pred_img_highres = pred_hr.squeeze().numpy()  # [H*2, W*2]\n\n    # Plot\n    fig, axs = plt.subplots(1, 3 if velocity_file else 1, figsize=(18, 6))\n    axs = axs if isinstance(axs, np.ndarray) else [axs]\n\n    axs[0].imshow(pred_img_highres, cmap=\"jet\", aspect=\"auto\")\n    axs[0].set_title(\"Predicted Velocity Map (Upscaled 2x)\")\n\n    if velocity_file:\n        ground_truth = np.load(velocity_file)\n        axs[1].imshow(ground_truth, cmap=\"jet\", aspect=\"auto\")\n        axs[1].set_title(\"Ground Truth Velocity Map\")\n\n        # Resize ground truth if needed for shape match\n        if ground_truth.shape != pred_img.shape:\n            gt_tensor = torch.tensor(ground_truth).unsqueeze(0).unsqueeze(0).float()\n            gt_resized = F.interpolate(gt_tensor, size=pred_img.shape, mode='bilinear', align_corners=False)\n            ground_truth = gt_resized.squeeze().numpy()\n\n        diff = np.abs(pred_img - ground_truth)\n        axs[2].imshow(diff, cmap=\"hot\", aspect=\"auto\")\n        axs[2].set_title(\"Prediction Error Map\")\n\n    print(\"Prediction shape:\", prediction.shape)\n    st.pyplot(fig)\n    st.success(\"Prediction complete and displayed successfully!\")\n\n\n\n# execute the app by running the following command in terminal from the directory where this script is located:\n#cd ConvNeXt-Final\n# streamlit run seismicApp.py","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}